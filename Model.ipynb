{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./Utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset function\n",
    "def load_dataset():\n",
    "    \"\"\"Load dataset from files\n",
    "    Returns\n",
    "    -------\n",
    "    train_X : array-like, shape (n_samples_train, n_features)  \n",
    "              with train samples\n",
    "    train_Y : array-like, shape (n_samples_train, 1) \n",
    "              with train labels\n",
    "    train_X : array-like, shape (n_samples_test, n_features) \n",
    "              with test samples\n",
    "    train_Y : array-like, shape (n_samples_test, 1) \n",
    "              with test labels\n",
    "    \"\"\"\n",
    "    train_X = np.load(f\"train_{COLLECTION_FILE_NAME}_x.npy\")\n",
    "    train_Y = np.load(f\"train_{COLLECTION_FILE_NAME}_y.npy\")\n",
    "    test_X = np.load(f\"test_{COLLECTION_FILE_NAME}_x.npy\")\n",
    "    test_Y = np.load(f\"test_{COLLECTION_FILE_NAME}_y.npy\")\n",
    "    return train_X, train_Y, test_X, test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 10304) (320,) (80, 10304) (80,)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset to work with\n",
    "X_train, Y_train, X_test, Y_test = load_dataset() \n",
    "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle function\n",
    "def shuffle_samples(x, y):\n",
    "    \"\"\"Shuffle data of labels and samples\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Samples, where n_samples is the number of samples\n",
    "        and n_features is the number of features.\n",
    "    y : array-like, shape (n_samples, 1)\n",
    "        Labels, where n_samples is the number of samples.\n",
    "    Returns\n",
    "    -------\n",
    "    data[:,:-1] : array-like, shape (n_samples, n_features) \n",
    "    data[:,-1]  : array-like, shape (n_samples, label)\n",
    "    \"\"\"\n",
    "    data = np.concatenate((x, y.reshape(-1,1)), axis=1)\n",
    "    np.random.shuffle(data)\n",
    "    return data[:,:-1] , data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 10304) (320,)\n"
     ]
    }
   ],
   "source": [
    "# Extra shuffle for samples\n",
    "X_train, Y_train = shuffle_samples(X_train, Y_train)\n",
    "print(X_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default scaling\n",
    "def minmaxscaler(samples):\n",
    "    \"\"\"Scale features by maximum of data\n",
    "    Parameters\n",
    "    ----------\n",
    "    samples : array-like, shape (n_samples, n_features)\n",
    "              Samples, where n_samples is the number of samples\n",
    "              and n_features is the number of features.\n",
    "    Returns\n",
    "    -------\n",
    "    samples : array-like, same shape as samples\n",
    "    \"\"\"\n",
    "    return samples/255.\n",
    "\n",
    "# Scale images A\n",
    "def scale_by_mean(samples, axis=0):\n",
    "    \"\"\"Scale features by mean of data depending on the axis value\n",
    "    Parameters\n",
    "    ----------\n",
    "    samples : array-like, shape (n_samples, n_features)\n",
    "              Samples, where n_samples is the number of samples\n",
    "              and n_features is the number of features.\n",
    "    axis    : number type of int(0, 1)\n",
    "    Returns\n",
    "    -------\n",
    "    samples : array-like, same shape as samples\n",
    "    \"\"\"\n",
    "    mean = np.mean(samples, axis=axis)\n",
    "    mean = mean.reshape(samples.shape[0], -1) if axis == 1 else mean\n",
    "    return samples - mean\n",
    "\n",
    "# Scale images B\n",
    "def scale_by_mean_std(samples, axis=0):\n",
    "    \"\"\"Scale features by mean diveded by standart deviation\n",
    "       of data depending on the axis value\n",
    "    Parameters\n",
    "    ----------\n",
    "    samples : array-like, shape (n_samples, n_features)\n",
    "              Samples, where n_samples is the number of samples\n",
    "              and n_features is the number of features.\n",
    "    axis    : number type of int(0, 1)\n",
    "    Returns\n",
    "    -------\n",
    "    samples : array-like, same shape as samples\n",
    "    \"\"\"\n",
    "    std = np.std(samples, axis=axis)\n",
    "    std = std.reshape(samples.shape[0], -1) if axis == 1 else std\n",
    "    return scale_by_mean(samples, axis)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 10304) 0.44243154666758017\n",
      "(320, 10304) 0.4415062551854981\n"
     ]
    }
   ],
   "source": [
    "# Scaling of samples\n",
    "X_train = minmaxscaler(X_train)\n",
    "X_test  = minmaxscaler(X_test)\n",
    "print(X_test.shape, X_test.mean())\n",
    "print(X_train.shape, X_train.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA realization\n",
    "def get_data(A):\n",
    "    \"\"\"Count centered matrix C\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : array-like, shape (n_samples, n_features)\n",
    "        Samples, where n_samples is the number of samples\n",
    "        and n_features is the number of features.\n",
    "    Returns\n",
    "    -------\n",
    "    C : array-like, same shape as samples\n",
    "    \"\"\"\n",
    "    # calculate the mean of each column\n",
    "    M = np.mean(A, axis=0)\n",
    "    # center columns by subtracting column means\n",
    "    C =  A - M\n",
    "    return C\n",
    "\n",
    "def get_svd(C):\n",
    "    \"\"\"Count singular value decomposition of matrix C\n",
    "    Method count explained variance and explained variance ratio\n",
    "    Parameters\n",
    "    ----------\n",
    "    C : array-like, shape (n_samples, n_features)\n",
    "        Samples, where n_samples is the number of samples\n",
    "        and n_features is the number of features.\n",
    "    Returns\n",
    "    -------\n",
    "    values  : array-like, shape (n_features,) diagonal matrix of singular values\n",
    "    vectors : array-like, shape (n_features, n_features)\n",
    "    explained_variance       : shape (n_features,) where n_features is \n",
    "                               a number of components\n",
    "    explained_variance_ratio : shape (n_features,) where n_features is \n",
    "                               a number of components\n",
    "    \"\"\"\n",
    "    # svd method\n",
    "    vectors, values,_ = np.linalg.svd(C.T)\n",
    "    \n",
    "    # count diagonal matrix of eigenvector\n",
    "    explained_variance = (values ** 2) / (C.shape[0] - 1)\n",
    "    \n",
    "    # count explained variance ratio     \n",
    "    total_var = explained_variance.sum()\n",
    "    explained_variance_ratio = explained_variance / total_var\n",
    "    \n",
    "    return values, vectors, explained_variance, explained_variance_ratio\n",
    "\n",
    "def explained_variance(values, ratio):\n",
    "    \"\"\" Count decision how many n components will be used\n",
    "    Parameters\n",
    "    ----------\n",
    "    values : array-like, shape (n_components,)\n",
    "             Explained variance ratio where n_components\n",
    "             is the number of components.\n",
    "    ratio  : float number, number in range (0, 1)\n",
    "    Returns\n",
    "    -------\n",
    "    container : array-like, shape(n_components_reduced,)\n",
    "                where n_components_reduced < n_components\n",
    "    \"\"\"\n",
    "    container = []\n",
    "    accumulator = 0. \n",
    "\n",
    "    for i in values:\n",
    "        if accumulator <= ratio:\n",
    "            accumulator += i\n",
    "            container.append(i)\n",
    "    \n",
    "    return np.asarray(container)\n",
    "\n",
    "def reduce_dimensions(matrix_W, n_comp, C):\n",
    "    \"\"\" Apply dimension reduction for matrix C\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix_W : array-like, shape (n_features, n_features)\n",
    "               Matrix W, where n_features is the number of features.\n",
    "    n_comp   : array-like, shape (n_components_reduced,) \n",
    "               Selected principal components, where n_components_reduced\n",
    "               is a number of components to use.\n",
    "    ะก        : array-like, shape (n_samples, n_features)\n",
    "               Samples, where n_samples is the number of samples\n",
    "               and n_features is the number of features.\n",
    "    Returns\n",
    "    -------\n",
    "    C : array-like, shape(n_samples, n_components_reduced)\n",
    "    \"\"\"\n",
    "    return C.dot(matrix_W.T[:n_comp.shape[0]].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit X_train\n",
    "train_matrix_C = get_data(X_train)\n",
    "eigen_values, eigen_vectors,exp_var, exp_ratio = get_svd(train_matrix_C)\n",
    "\n",
    "exp_ratio_n_comp = explained_variance(exp_ratio, .95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 161)\n"
     ]
    }
   ],
   "source": [
    "# Transform X_train\n",
    "X_train_PCA = reduce_dimensions(eigen_vectors, exp_ratio_n_comp, train_matrix_C)\n",
    "print(X_train_PCA.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 161)\n"
     ]
    }
   ],
   "source": [
    "# Transform X_test\n",
    "test_matrix_C = get_data(X_test)\n",
    "\n",
    "X_test_PCA = reduce_dimensions(eigen_vectors, exp_ratio_n_comp, test_matrix_C)\n",
    "print(X_test_PCA.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model realization\n",
    "from scipy import stats\n",
    "\n",
    "def euclidean_distance(q, p):\n",
    "    \"\"\"Count Euclidean Distance\n",
    "    Parameters\n",
    "    ----------\n",
    "    q : array-like, shape (n_samples, n_features)\n",
    "        Samples, where n_samples is the number of samples\n",
    "        and n_features is the number of features.\n",
    "    p : array-like, shape (n_features,)\n",
    "        Sample, where n_features is the number of features\n",
    "    Returns\n",
    "    -------\n",
    "    array-like, shape (n_samples,)\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum((q - p)**2, axis=1))\n",
    "    \n",
    "def model(X_train, y_train, X_test, y_test, K):\n",
    "    \"\"\"Build classificator\n",
    "    Return accuracy of predictions\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : array-like, shape (n_samples, n_features)\n",
    "              Samples, where n_samples is the number of samples\n",
    "              and n_features is the number of features.\n",
    "    y_train : array-like, shape (n_samples,)\n",
    "              Labels, where n_samples is the number of samples\n",
    "    X_test  : array-like, shape (n_samples, n_features)\n",
    "              Samples, where n_samples is the number of samples\n",
    "              and n_features is the number of features.\n",
    "    y_train : array-like, shape (n_samples,)\n",
    "              Labels, where n_samples is the number of samples\n",
    "    K       : int number, number for knn algorithm \n",
    "    Returns\n",
    "    -------\n",
    "    preidct : float number, precision of classification\n",
    "    \"\"\"\n",
    "    predict = np.zeros(X_test.shape[0])\n",
    "    for i in range(X_test.shape[0]):\n",
    "        euclid_dist = euclidean_distance(X_train, X_test[i])\n",
    "        indices = np.argsort(euclid_dist)[:K]\n",
    "        \n",
    "        predict[i] = stats.mode(y_train[indices])[0] == y_test[i]\n",
    "    return predict.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9375"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = model(X_train_PCA, Y_train, X_test_PCA, Y_test.reshape(-1, 1), 1)\n",
    "d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
